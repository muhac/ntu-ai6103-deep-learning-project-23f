{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HOsXceG55Z6Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import random\n",
        "import pdb\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setings"
      ],
      "metadata": {
        "id": "HzcxCNjrAXu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remember to check the epoch, testing epoch is 200,\n",
        "learning_rate = 0.01\n",
        "epochs = 2\n",
        "batch_size = 128\n",
        "senet_r = 4\n",
        "\n",
        "info = f\"r_{senet_r}_lr_{learning_rate}_batch_{batch_size}_epoch_{epochs}\"\n",
        "print(info)\n",
        "\n",
        "acc_and_loss_saved_filname = f\"experiment2_senet_{info}.json\"\n",
        "model_weight_filename = f\"experiment2_senet_{info}.pth\""
      ],
      "metadata": {
        "id": "0rIlzotOkGaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "614b77f1-3634-47fd-968e-308d20fd047f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r_4_lr_0.01_batch_128_epoch_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "\n",
        "# Set seed for PyTorch\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "# Set seed for NumPy\n",
        "np.random.seed(seed)\n",
        "# Set seed for Python's random module\n",
        "random.seed(seed)"
      ],
      "metadata": {
        "id": "x7Xn-ylHiDAl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "dPq9R6pKAfuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet50(nn.Module):\n",
        "    def __init__(self, block, num_classes=1000, r=1):\n",
        "        super(ResNet50, self).__init__()\n",
        "        self.block = block\n",
        "        self.r = r\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.stage1 = self.stage_input()\n",
        "        self.stage2 = self.stage_blocks(3, 64, 64, stride=1)\n",
        "        self.stage3 = self.stage_blocks(4, 64 * 4, 128, stride=2)\n",
        "        self.stage4 = self.stage_blocks(6, 128 * 4, 256, stride=2)\n",
        "        self.stage5 = self.stage_blocks(3, 256 * 4, 512, stride=2)\n",
        "        self.stage6 = self.stage_output(512 * 4, self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stage1(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "        x = self.stage5(x)\n",
        "        x = self.stage6(x)\n",
        "        return x\n",
        "\n",
        "    def stage_input(self):\n",
        "        conv = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        bn = nn.BatchNorm2d(64)\n",
        "        relu = nn.ReLU()\n",
        "        pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        return nn.Sequential(conv, bn, relu, pool)\n",
        "\n",
        "    def stage_blocks(self, n, in_channels, out_channels, stride=1):\n",
        "        layers = [self.block(in_channels, out_channels, stride, self.r)]\n",
        "        for i in range(n - 1):\n",
        "            layers.append(self.block(out_channels * 4, out_channels, self.r))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def stage_output(self, in_channels, out_channels, stride=1):\n",
        "        pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        flatten = nn.Flatten()\n",
        "        fc = nn.Linear(in_channels, out_channels)\n",
        "        return nn.Sequential(pool, flatten, fc)\n",
        "\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, r=1):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * 4, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels * 4)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.need_downsample = stride != 1 or in_channels != out_channels * 4\n",
        "        if self.need_downsample:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * 4, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * 4)\n",
        "            )\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.bn3(self.conv3(x))\n",
        "\n",
        "        if self.need_downsample:\n",
        "            identity = self.downsample(identity)\n",
        "\n",
        "        out = x + identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channel, r):\n",
        "        super().__init__()\n",
        "        reductionChannel = int(channel / r)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc1 = nn.Linear(channel, reductionChannel)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(reductionChannel, channel)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # pdb.set_trace()\n",
        "        out = self.pool(x)\n",
        "        batch, channel = out.size(0), out.size(1)\n",
        "        out = out.view(batch, -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = out.view(batch, channel, 1, 1)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SEResNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, r=1):\n",
        "        super(SEResNetBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * 4, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels * 4)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.need_downsample = stride != 1 or in_channels != out_channels * 4\n",
        "        if self.need_downsample:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * 4, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * 4)\n",
        "            )\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "        self.seBlock = SEBlock(out_channels * 4, r)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.bn3(self.conv3(x))\n",
        "\n",
        "        if self.need_downsample:\n",
        "            identity = self.downsample(identity)\n",
        "\n",
        "        # apply SE\n",
        "        scale = self.seBlock(x)\n",
        "\n",
        "        out = x * scale + identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "yp4v9Qi0_2GO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "kJttW4d2AjOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "# Download CIFAR-10 training dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Download CIFAR-10 testing dataset\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8FFk5CNY64k",
        "outputId": "55ce3370-6bf4-4a4b-ae4a-8392e90157bd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, valid = torch.utils.data.random_split(train_dataset, [40000, 10000])\n",
        "\n",
        "# Create data loaders for training and testing datasets\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "mpRSy-Wy-ExW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "URx0yMRpAv_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet50(SEResNetBlock, r=senet_r)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "y0EzX5awY3Pa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, train_acc = [], []\n",
        "valid_loss, valid_acc = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Train loop\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # train\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # saving for loss and acc\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_samples += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss.append(running_loss)\n",
        "    train_acc.append(correct_predictions / total_samples)\n",
        "    scheduler.step()\n",
        "\n",
        "    # Valid loop\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # test\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # saving for loss and acc\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    valid_loss.append(running_loss)\n",
        "    valid_acc.append(correct_predictions / total_samples)\n",
        "\n",
        "    print(\n",
        "        f'Epoch {epoch + 1}/{epochs} - '\n",
        "        f'Train Running Loss: {train_loss[-1]:.4f} - '\n",
        "        f'Valid Loss: {valid_loss[-1]:.4f} - '\n",
        "        f'Valid Accuracy: {valid_acc[-1]:.4f}'\n",
        "    )\n",
        "\n",
        "print('Training finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsFbOBYfYRkF",
        "outputId": "668505a1-a316-4efc-a6df-3957badd0e1b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2 - Train Running Loss: 777.2864 - Valid Loss: 189.3143 - Valid Accuracy: 0.1324\n",
            "Epoch 2/2 - Train Running Loss: 687.2130 - Valid Loss: 176.8077 - Valid Accuracy: 0.1674\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "loU89NQHAykk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing loop\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "running_loss = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # test\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # saving for loss and acc\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_samples += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "print(\n",
        "    f'Epoch {epoch + 1}/{epochs} - '\n",
        "    f'Train Running Loss: {train_loss[-1]:.4f} - '\n",
        "    f'Test Loss: {running_loss:.4f} - '\n",
        "    f'Test Accuracy: {correct_predictions / total_samples:.4f}'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr1PxQYb_G9l",
        "outputId": "24a377b9-3607-4288-acd5-de348582466c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/2 - Train Running Loss: 687.2130 - Test Loss: 177.1053 - Test Accuracy: 0.1702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Data"
      ],
      "metadata": {
        "id": "GsFvw8InACJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, model_weight_filename)"
      ],
      "metadata": {
        "id": "-M-5mZN4-m_N"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_loss_and_acc = {\n",
        "    \"train_loss\": train_loss,\n",
        "    \"train_acc\": train_acc,\n",
        "    \"valid_loss\": valid_loss,\n",
        "    \"valid_acc\": valid_acc,\n",
        "}\n",
        "\n",
        "with open(acc_and_loss_saved_filname, 'w') as json_file:\n",
        "    json.dump(saved_loss_and_acc, json_file)\n"
      ],
      "metadata": {
        "id": "7aQRMehClH7A"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W5ICpcM9APjQ"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}